/*
 * Speechmatics ASR REST API
 *
 * The Speechmatics Automatic Speech Recognition REST API is used to submit ASR jobs and receive the results. 
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: support@speechmatics.com
 * Generated by: https://openapi-generator.tech
 */

use crate::batch::models;
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct TranscriptionConfig {
    /// Language model to process the audio input, normally specified as an ISO language code
    #[serde(rename = "language")]
    pub language: String,
    /// Request a specialized model based on 'language' but optimized for a particular field, e.g. \"finance\" or \"medical\".
    #[serde(rename = "domain", skip_serializing_if = "Option::is_none")]
    pub domain: Option<String>,
    /// Language locale to be used when generating the transcription output, normally specified as an ISO language code
    #[serde(rename = "output_locale", skip_serializing_if = "Option::is_none")]
    pub output_locale: Option<String>,
    #[serde(rename = "operating_point", skip_serializing_if = "Option::is_none")]
    pub operating_point: Option<models::OperatingPoint>,
    /// List of custom words or phrases that should be recognized. Alternative pronunciations can be specified to aid recognition.
    #[serde(rename = "additional_vocab", skip_serializing_if = "Option::is_none")]
    pub additional_vocab: Option<Vec<models::TranscriptionConfigAdditionalVocabInner>>,
    #[serde(rename = "punctuation_overrides", skip_serializing_if = "Option::is_none")]
    pub punctuation_overrides: Option<Box<models::TranscriptionConfigPunctuationOverrides>>,
    /// Specify whether speaker or channel labels are added to the transcript. The default is `none`.   - **none**: no speaker or channel labels are added.   - **speaker**: speaker attribution is performed based on acoustic matching;              all input channels are mixed into a single stream for processing.   - **channel**: multiple input channels are processed individually and collated             into a single transcript.
    #[serde(rename = "diarization", skip_serializing_if = "Option::is_none")]
    pub diarization: Option<Diarization>,
    /// Transcript labels to use when using collating separate input channels.
    #[serde(rename = "channel_diarization_labels", skip_serializing_if = "Option::is_none")]
    pub channel_diarization_labels: Option<Vec<String>>,
    /// Include additional 'entity' objects in the transcription results (e.g. dates, numbers) and their original spoken form. These entities are interleaved with other types of results. The concatenation of these words is represented as a single entity with the concatenated written form present in the 'content' field. The entities contain a 'spoken_form' field, which can be used in place of the corresponding 'word' type results, in case a spoken form is preferred to a written form. They also contain a 'written_form', which can be used instead of the entity, if you want a breakdown of the words without spaces. They can still contain non-breaking spaces and other special whitespace characters, as they are considered part of the word for the formatting output. In case of a written_form, the individual word times are estimated and might not be accurate if the order of the words in the written form does not correspond to the order they were actually spoken (such as 'one hundred million dollars' and '$100 million').
    #[serde(rename = "enable_entities", skip_serializing_if = "Option::is_none")]
    pub enable_entities: Option<bool>,
    /// Whether or not to enable flexible endpointing and allow the entity to continue to be spoken.
    #[serde(rename = "max_delay_mode", skip_serializing_if = "Option::is_none")]
    pub max_delay_mode: Option<MaxDelayMode>,
    #[serde(rename = "speaker_diarization_config", skip_serializing_if = "Option::is_none")]
    pub speaker_diarization_config: Option<Box<models::TranscriptionConfigSpeakerDiarizationConfig>>,
}

impl TranscriptionConfig {
    pub fn new(language: String) -> TranscriptionConfig {
        TranscriptionConfig {
            language,
            domain: None,
            output_locale: None,
            operating_point: None,
            additional_vocab: None,
            punctuation_overrides: None,
            diarization: None,
            channel_diarization_labels: None,
            enable_entities: None,
            max_delay_mode: None,
            speaker_diarization_config: None,
        }
    }
}
/// Specify whether speaker or channel labels are added to the transcript. The default is `none`.   - **none**: no speaker or channel labels are added.   - **speaker**: speaker attribution is performed based on acoustic matching;              all input channels are mixed into a single stream for processing.   - **channel**: multiple input channels are processed individually and collated             into a single transcript.
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum Diarization {
    #[serde(rename = "none")]
    None,
    #[serde(rename = "speaker")]
    Speaker,
    #[serde(rename = "channel")]
    Channel,
}

impl Default for Diarization {
    fn default() -> Diarization {
        Self::None
    }
}
/// Whether or not to enable flexible endpointing and allow the entity to continue to be spoken.
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum MaxDelayMode {
    #[serde(rename = "fixed")]
    Fixed,
    #[serde(rename = "flexible")]
    Flexible,
}

impl Default for MaxDelayMode {
    fn default() -> MaxDelayMode {
        Self::Fixed
    }
}

